Here are a few copy/paste Snowflake SQL patterns that will give you daily spend across warehouses, while excluding Document AI (both the Snowsight “Document AI” feature and the Cortex document-processing functions like PARSE_DOCUMENT, AI_EXTRACT, <model_build_name>!PREDICT).  ￼

⸻

1) Daily warehouse compute credits (incl. idle) excluding Document AI

This gives you a clean per-warehouse daily view showing:
	•	total compute credits (includes idle),
	•	idle compute credits,
	•	Document AI query execution credits,
	•	“non-Document-AI” query execution credits,
	•	and “non-Document-AI compute including idle”.

-- Tip: if you plan to reconcile with ORG views later, consider UTC:
-- ALTER SESSION SET TIMEZONE = 'UTC';

WITH params AS (
  SELECT DATEADD('day', -30, CURRENT_TIMESTAMP()) AS start_ts
),

-- All query IDs that are "Document AI"
doc_ai_query_ids AS (
  SELECT DISTINCT query_id
  FROM SNOWFLAKE.ACCOUNT_USAGE.DOCUMENT_AI_USAGE_HISTORY
  WHERE start_time >= (SELECT start_ts FROM params)

  UNION

  SELECT DISTINCT query_id
  FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_DOCUMENT_PROCESSING_USAGE_HISTORY
  WHERE start_time >= (SELECT start_ts FROM params)
),

-- Compute credits attributed to Document AI queries (execution only; no idle)
doc_ai_attributed AS (
  SELECT
    DATE_TRUNC('day', qah.start_time) AS usage_day,
    qah.warehouse_name,
    SUM(COALESCE(qah.credits_attributed_compute, 0)) AS docai_query_exec_credits,
    SUM(COALESCE(qah.credits_used_query_acceleration, 0)) AS docai_qas_credits
  FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY qah
  WHERE qah.start_time >= (SELECT start_ts FROM params)
    AND (
      qah.query_id IN (SELECT query_id FROM doc_ai_query_ids)
      OR COALESCE(qah.root_query_id, qah.query_id) IN (SELECT query_id FROM doc_ai_query_ids)
    )
  GROUP BY 1, 2
),

-- Warehouse daily totals
wh_daily AS (
  SELECT
    DATE_TRUNC('day', start_time) AS usage_day,
    warehouse_name,
    SUM(credits_used_compute) AS total_compute_credits,
    SUM(credits_used_cloud_services) AS total_cloud_services_credits,
    SUM(credits_used) AS total_credits,
    SUM(credits_attributed_compute_queries) AS total_attributed_query_credits
  FROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY
  WHERE start_time >= (SELECT start_ts FROM params)
  GROUP BY 1, 2
)

SELECT
  w.usage_day,
  w.warehouse_name,

  -- Totals
  w.total_compute_credits,
  w.total_attributed_query_credits,
  (w.total_compute_credits - w.total_attributed_query_credits) AS idle_compute_credits,

  -- Doc AI subset (execution only)
  COALESCE(d.docai_query_exec_credits, 0) AS docai_query_exec_credits,

  -- What you asked for: "non Document AI" across warehouses
  (w.total_attributed_query_credits - COALESCE(d.docai_query_exec_credits, 0)) AS non_docai_query_exec_credits,
  (w.total_compute_credits - COALESCE(d.docai_query_exec_credits, 0)) AS non_docai_compute_including_idle_credits,

  -- Useful context
  w.total_cloud_services_credits,
  COALESCE(d.docai_qas_credits, 0) AS docai_qas_credits

FROM wh_daily w
LEFT JOIN doc_ai_attributed d
  ON w.usage_day = d.usage_day
 AND w.warehouse_name = d.warehouse_name
ORDER BY 1, 2;

Why this works:
	•	WAREHOUSE_METERING_HISTORY gives hourly credits used (compute + cloud services) plus credits attributed to queries (so you can compute idle).  ￼
	•	QUERY_ATTRIBUTION_HISTORY gives per-query compute credits (execution only, not idle), which makes “exclude Document AI queries” feasible.  ￼
	•	Both Document AI views provide the query IDs to exclude.  ￼

Notes:
	•	Warehouse metering credits don’t include the daily cloud-services billing adjustment; billed compute is best reconciled via METERING_DAILY_HISTORY (account-level).  ￼
	•	Latency: warehouse metering can lag hours; query attribution can lag up to ~8 hours.  ￼

⸻

2) Daily AI token credits (Cortex AI Functions) by warehouse excluding Document AI

If by “expenditure on non Document AI processes” you meant AI-services credits (token-based), this uses CORTEX_AISQL_USAGE_HISTORY and removes any query IDs that were document processing. TOKEN_CREDITS explicitly excludes warehouse credits.  ￼

WITH params AS (
  SELECT DATEADD('day', -30, CURRENT_TIMESTAMP()) AS start_ts
),

doc_ai_query_ids AS (
  SELECT DISTINCT query_id
  FROM SNOWFLAKE.ACCOUNT_USAGE.DOCUMENT_AI_USAGE_HISTORY
  WHERE start_time >= (SELECT start_ts FROM params)

  UNION

  SELECT DISTINCT query_id
  FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_DOCUMENT_PROCESSING_USAGE_HISTORY
  WHERE start_time >= (SELECT start_ts FROM params)
),

wh_map AS (
  SELECT DISTINCT warehouse_id::TEXT AS warehouse_id, warehouse_name
  FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY
  WHERE start_time >= (SELECT start_ts FROM params)
    AND warehouse_id IS NOT NULL
    AND warehouse_name IS NOT NULL
),

aisql_non_doc AS (
  SELECT
    DATE_TRUNC('day', usage_time) AS usage_day,
    warehouse_id,
    SUM(token_credits) AS ai_token_credits,
    SUM(tokens) AS tokens
  FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_AISQL_USAGE_HISTORY
  WHERE usage_time >= (SELECT start_ts FROM params)
    AND query_id NOT IN (SELECT query_id FROM doc_ai_query_ids)
  GROUP BY 1, 2
)

SELECT
  a.usage_day,
  m.warehouse_name,
  a.ai_token_credits,
  a.tokens
FROM aisql_non_doc a
LEFT JOIN wh_map m
  ON a.warehouse_id::TEXT = m.warehouse_id
ORDER BY 1, 2;

Important gotcha: CORTEX_AISQL_USAGE_HISTORY only includes usage on/after Nov 17, 2025.  ￼

⸻

3) Convert credits → currency (if you have ORG usage access)

If you can query SNOWFLAKE.ORGANIZATION_USAGE.USAGE_IN_CURRENCY_DAILY, you can compute an effective $/credit for WAREHOUSE_METERING and multiply your per-warehouse credits by it. (This view reports usage date in UTC and can lag up to 72 hours.)  ￼

WITH price_per_credit AS (
  SELECT
    usage_date,
    account_locator,
    SUM(CASE WHEN rating_type='compute'
              AND service_type='WAREHOUSE_METERING'
              AND NOT is_adjustment
             THEN usage_in_currency END)
    /
    NULLIF(SUM(CASE WHEN rating_type='compute'
                     AND service_type='WAREHOUSE_METERING'
                     AND NOT is_adjustment
                    THEN usage END), 0) AS currency_per_credit,
    MAX(currency) AS currency
  FROM SNOWFLAKE.ORGANIZATION_USAGE.USAGE_IN_CURRENCY_DAILY
  WHERE usage_date >= DATEADD('day', -30, CURRENT_DATE())
    AND account_locator = CURRENT_ACCOUNT()
  GROUP BY 1, 2
)
SELECT * FROM price_per_credit ORDER BY usage_date;

Then join price_per_credit onto the daily warehouse output from query #1 on date.

⸻

If you tell me which “expenditure” you care about most (warehouse credits, AI token credits, or true billed $), I’ll collapse this into one final query that outputs exactly the daily columns you want.






-- Change window as needed
WITH params AS (
  SELECT DATEADD('day', -30, CURRENT_TIMESTAMP()) AS start_ts
),

/* 1) Document AI jobs (Snowsight Document AI / job-style usage) */
docai_jobs AS (
  SELECT
    DATE_TRUNC('day', start_time) AS usage_day,
    query_id,
    operation_name,
    SUM(credits_used) AS docai_job_credits,
    SUM(page_count)   AS pages,
    SUM(document_count) AS documents,
    SUM(feature_count)  AS features
  FROM SNOWFLAKE.ACCOUNT_USAGE.DOCUMENT_AI_USAGE_HISTORY
  WHERE start_time >= (SELECT start_ts FROM params)
  GROUP BY 1,2,3
),

/* 2) Cortex Document Processing functions (PARSE_DOCUMENT / AI_EXTRACT / <model_build>!PREDICT, etc.) */
docai_funcs AS (
  SELECT
    DATE_TRUNC('day', start_time) AS usage_day,
    query_id,
    function_name,
    model_name,
    operation_name,
    SUM(credits_used) AS docai_func_credits,
    SUM(page_count)   AS pages,
    SUM(document_count) AS documents,
    SUM(feature_count)  AS features
  FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_DOCUMENT_PROCESSING_USAGE_HISTORY
  WHERE start_time >= (SELECT start_ts FROM params)
  GROUP BY 1,2,3,4,5
),

/* Set of query_ids that correspond to Document AI activity */
docai_query_ids AS (
  SELECT DISTINCT query_id FROM docai_jobs
  UNION
  SELECT DISTINCT query_id FROM docai_funcs
),

/* Warehouse/user attribution for those queries (execution credits only; no idle) */
wh_exec AS (
  SELECT
    DATE_TRUNC('day', start_time) AS usage_day,
    warehouse_name,
    user_name,
    SUM(COALESCE(credits_attributed_compute, 0)) AS wh_exec_credits,
    SUM(COALESCE(credits_used_query_acceleration, 0)) AS qas_credits
  FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY
  WHERE start_time >= (SELECT start_ts FROM params)
    AND (
      query_id IN (SELECT query_id FROM docai_query_ids)
      OR COALESCE(root_query_id, query_id) IN (SELECT query_id FROM docai_query_ids)
    )
  GROUP BY 1,2,3
),

/* Attach warehouse/user to AI credits via attribution (best-effort) */
ai_credits_by_wh AS (
  SELECT
    e.usage_day,
    e.warehouse_name,
    e.user_name,
    COALESCE(SUM(j.docai_job_credits), 0) AS docai_job_credits,
    COALESCE(SUM(f.docai_func_credits), 0) AS docai_func_credits
  FROM wh_exec e
  LEFT JOIN docai_jobs  j ON e.usage_day = j.usage_day AND e.user_name IS NOT NULL AND j.query_id IS NOT NULL AND j.query_id IN (SELECT query_id FROM docai_query_ids)
  LEFT JOIN docai_funcs f ON e.usage_day = f.usage_day AND e.user_name IS NOT NULL AND f.query_id IS NOT NULL AND f.query_id IN (SELECT query_id FROM docai_query_ids)
  GROUP BY 1,2,3
)

SELECT
  e.usage_day,
  e.warehouse_name,
  e.user_name,

  -- AI service credits (Document AI)
  COALESCE(a.docai_job_credits, 0)  AS docai_job_credits,   -- from DOCUMENT_AI_USAGE_HISTORY
  COALESCE(a.docai_func_credits, 0) AS docai_func_credits,  -- from CORTEX_DOCUMENT_PROCESSING_USAGE_HISTORY
  (COALESCE(a.docai_job_credits, 0) + COALESCE(a.docai_func_credits, 0)) AS docai_ai_service_credits,

  -- Warehouse compute used to run the SQL queries that triggered Document AI (execution only)
  e.wh_exec_credits,
  e.qas_credits,

  (COALESCE(a.docai_job_credits, 0) + COALESCE(a.docai_func_credits, 0) + e.wh_exec_credits + e.qas_credits) AS docai_total_credits_est

FROM wh_exec e
LEFT JOIN ai_credits_by_wh a
  ON e.usage_day = a.usage_day
 AND e.warehouse_name = a.warehouse_name
 AND e.user_name = a.user_name
ORDER BY 1,2,3;









-- ============
-- PARAMETERS
-- ============
ALTER SESSION SET TIMEZONE = 'UTC';

SET START_DATE = '2025-11-17'::DATE;
SET END_DATE   = '2025-12-03'::DATE;

-- Your conversion:
SET USD_PER_CREDIT = 3;

-- Optional if you want a rough storage $ estimate (Snowflake storage is usually $/TB-month, not credits).
-- Leave NULL if unknown.
SET STORAGE_USD_PER_TB_MONTH = NULL;  -- e.g. 40

WITH params AS (
  SELECT
    $START_DATE::DATE AS start_date,
    $END_DATE::DATE   AS end_date,
    TO_TIMESTAMP_LTZ($START_DATE::DATE) AS start_ts,
    TO_TIMESTAMP_LTZ(DATEADD('day', 1, $END_DATE::DATE)) AS end_ts,
    $USD_PER_CREDIT::NUMBER(38,9) AS usd_per_credit,
    $STORAGE_USD_PER_TB_MONTH::NUMBER(38,9) AS storage_usd_per_tb_month
),

-- Map warehouse_id -> warehouse_name for the window (for AISQL view which provides WAREHOUSE_ID)
wh_map AS (
  SELECT DISTINCT
    warehouse_id::TEXT AS warehouse_id,
    warehouse_name
  FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY
  WHERE start_time >= (SELECT start_ts FROM params)
    AND start_time <  (SELECT end_ts   FROM params)
    AND warehouse_id IS NOT NULL
    AND warehouse_name IS NOT NULL
),

-- -------------------------
-- Document AI credits (serverless)
-- -------------------------
doc_ai AS (
  SELECT
    COALESCE(q.warehouse_name, 'UNKNOWN') AS warehouse_name,
    SUM(d.credits_used) AS doc_ai_job_credits
  FROM SNOWFLAKE.ACCOUNT_USAGE.DOCUMENT_AI_USAGE_HISTORY d
  LEFT JOIN SNOWFLAKE.ACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY q
    ON d.query_id = q.query_id
   AND q.start_time >= (SELECT start_ts FROM params)
   AND q.start_time <  (SELECT end_ts   FROM params)
  WHERE d.start_time >= (SELECT start_ts FROM params)
    AND d.start_time <  (SELECT end_ts   FROM params)
  GROUP BY 1
),
doc_proc AS (
  SELECT
    COALESCE(q.warehouse_name, 'UNKNOWN') AS warehouse_name,
    SUM(p.credits_used) AS doc_processing_func_credits
  FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_DOCUMENT_PROCESSING_USAGE_HISTORY p
  LEFT JOIN SNOWFLAKE.ACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY q
    ON p.query_id = q.query_id
   AND q.start_time >= (SELECT start_ts FROM params)
   AND q.start_time <  (SELECT end_ts   FROM params)
  WHERE p.start_time >= (SELECT start_ts FROM params)
    AND p.start_time <  (SELECT end_ts   FROM params)
  GROUP BY 1
),
doc_ai_by_wh AS (
  SELECT
    COALESCE(a.warehouse_name, b.warehouse_name) AS warehouse_name,
    COALESCE(a.doc_ai_job_credits, 0) + COALESCE(b.doc_processing_func_credits, 0) AS doc_ai_total_credits
  FROM doc_ai a
  FULL OUTER JOIN doc_proc b
    ON a.warehouse_name = b.warehouse_name
),

-- -------------------------
-- Cortex AI COMPLETE credits (token credits; not warehouse compute)
-- -------------------------
cortex_complete_by_wh AS (
  SELECT
    COALESCE(m.warehouse_name, 'UNKNOWN') AS warehouse_name,
    SUM(u.token_credits) AS cortex_complete_credits
  FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_AISQL_USAGE_HISTORY u
  LEFT JOIN wh_map m
    ON u.warehouse_id = m.warehouse_id
  WHERE u.usage_time >= (SELECT start_ts FROM params)
    AND u.usage_time <  (SELECT end_ts   FROM params)
    AND UPPER(u.function_name) = 'COMPLETE'
  GROUP BY 1
),

-- -------------------------
-- Build set of "AI-involved" query_ids (any Cortex AI function + any DocAI/Doc processing)
-- Used to exclude those queries from "non-AI SQL compute" bucket
-- -------------------------
ai_query_ids AS (
  SELECT DISTINCT query_id
  FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_AISQL_USAGE_HISTORY
  WHERE usage_time >= (SELECT start_ts FROM params)
    AND usage_time <  (SELECT end_ts   FROM params)

  UNION

  SELECT DISTINCT query_id
  FROM SNOWFLAKE.ACCOUNT_USAGE.DOCUMENT_AI_USAGE_HISTORY
  WHERE start_time >= (SELECT start_ts FROM params)
    AND start_time <  (SELECT end_ts   FROM params)

  UNION

  SELECT DISTINCT query_id
  FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_DOCUMENT_PROCESSING_USAGE_HISTORY
  WHERE start_time >= (SELECT start_ts FROM params)
    AND start_time <  (SELECT end_ts   FROM params)
),

-- -------------------------
-- Non-AI SQL query compute (execution only) excluding AI/DocAI query_ids (and their root)
-- -------------------------
non_ai_sql_by_wh AS (
  SELECT
    warehouse_name,
    SUM(COALESCE(credits_attributed_compute, 0)) AS non_ai_query_exec_credits,
    SUM(COALESCE(credits_used_query_acceleration, 0)) AS non_ai_qas_credits
  FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY
  WHERE start_time >= (SELECT start_ts FROM params)
    AND start_time <  (SELECT end_ts   FROM params)
    AND warehouse_name IS NOT NULL
    AND COALESCE(root_query_id, query_id) NOT IN (SELECT query_id FROM ai_query_ids)
    AND query_id NOT IN (SELECT query_id FROM ai_query_ids)
  GROUP BY 1
),

-- -------------------------
-- Account storage (not per warehouse)
-- -------------------------
acct_storage AS (
  SELECT
    AVG(
      COALESCE(storage_bytes,0)
    + COALESCE(stage_bytes,0)
    + COALESCE(failsafe_bytes,0)
    + COALESCE(hybrid_table_storage_bytes,0)
    ) AS avg_daily_total_bytes
  FROM SNOWFLAKE.ACCOUNT_USAGE.STORAGE_USAGE
  WHERE usage_date >= (SELECT start_date FROM params)
    AND usage_date <= (SELECT end_date   FROM params)
),

-- Union of warehouses appearing in any bucket
all_wh AS (
  SELECT warehouse_name FROM doc_ai_by_wh
  UNION
  SELECT warehouse_name FROM cortex_complete_by_wh
  UNION
  SELECT warehouse_name FROM non_ai_sql_by_wh
)

-- ============
-- FINAL OUTPUT
-- ============
SELECT
  'WAREHOUSE' AS row_type,
  w.warehouse_name,

  -- Credits
  COALESCE(c.cortex_complete_credits, 0) AS cortex_complete_credits,
  COALESCE(d.doc_ai_total_credits, 0)    AS document_ai_processing_credits,
  (COALESCE(n.non_ai_query_exec_credits, 0) + COALESCE(n.non_ai_qas_credits, 0)) AS non_ai_sql_compute_credits,

  -- USD (using your $/credit)
  COALESCE(c.cortex_complete_credits, 0) * (SELECT usd_per_credit FROM params) AS cortex_complete_usd,
  COALESCE(d.doc_ai_total_credits, 0)    * (SELECT usd_per_credit FROM params) AS document_ai_processing_usd,
  (COALESCE(n.non_ai_query_exec_credits, 0) + COALESCE(n.non_ai_qas_credits, 0))
    * (SELECT usd_per_credit FROM params) AS non_ai_sql_compute_usd,

  -- Totals
  (
    COALESCE(c.cortex_complete_credits, 0)
  + COALESCE(d.doc_ai_total_credits, 0)
  + COALESCE(n.non_ai_query_exec_credits, 0)
  + COALESCE(n.non_ai_qas_credits, 0)
  ) AS total_credits_est,
  (
    COALESCE(c.cortex_complete_credits, 0)
  + COALESCE(d.doc_ai_total_credits, 0)
  + COALESCE(n.non_ai_query_exec_credits, 0)
  + COALESCE(n.non_ai_qas_credits, 0)
  ) * (SELECT usd_per_credit FROM params) AS total_usd_est,

  -- Storage fields (not warehouse attributable)
  NULL::NUMBER AS avg_daily_storage_bytes,
  NULL::NUMBER AS approx_storage_usd_for_range

FROM all_wh w
LEFT JOIN cortex_complete_by_wh c ON w.warehouse_name = c.warehouse_name
LEFT JOIN doc_ai_by_wh d          ON w.warehouse_name = d.warehouse_name
LEFT JOIN non_ai_sql_by_wh n      ON w.warehouse_name = n.warehouse_name

UNION ALL

SELECT
  'ACCOUNT_STORAGE' AS row_type,
  '__ACCOUNT__' AS warehouse_name,
  NULL, NULL, NULL,
  NULL, NULL, NULL,
  NULL, NULL,
  s.avg_daily_total_bytes AS avg_daily_storage_bytes,
  /* Optional rough estimate:
     avg_daily_TB * $/TB-month * (#days / 30.4375)
  */
  CASE
    WHEN (SELECT storage_usd_per_tb_month FROM params) IS NULL THEN NULL
    ELSE
      (s.avg_daily_total_bytes / POW(1024, 4))
      * (SELECT storage_usd_per_tb_month FROM params)
      * (DATEDIFF('day', (SELECT start_date FROM params), DATEADD('day',1,(SELECT end_date FROM params))) / 30.4375)
  END AS approx_storage_usd_for_range
FROM acct_storage s

ORDER BY row_type, warehouse_name;
